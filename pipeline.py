# -*- coding: utf-8 -*-
"""demo_yc_uganda.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19dKm5BOZU5Wq5j6F_hRCTO0psN6j04wq

# Installations & Env Setup
"""

import pandas as pd
import os
from utils.pipeline_utils import load_pdf, extract_all_sections, load_xls, sheet_to_text, map_section_to_page_number, parquet_viewer, run_code_and_capture_df
import google.generativeai as genai
import nest_asyncio
import asyncio

from langchain_templates import info_extraction_prompt_template, translation_prompt_template, field_selection_prompt_template, aggregation_code_gen_template
from models.models import Category
from models.return_models import ColumnMapping, ColumnSelectionMapping, FunctionalCode
from instructions import column_mapping_instructions, field_selection_instructions, aggregation_instructions

from langchain.chat_models import init_chat_model

from modules.p1 import analyze_datasets, find_relevant_datasets

UPLOAD_FOLDER = os.getenv("UPLOAD_FOLDER") or 'uploads'
SUMMARY_PATH = 'summary.csv'
STATIC_FOLDER = os.getenv("STATIC_FOLDER") or 'static'

def run_part_1_2_module_field_selection(file_paths, output_dir):
  """ Check inputs  """
  for path in file_paths:
        print(f"Processing {path}...")

  """######## ------ PART 2: RELEVANT MODULE SELECTION ------- #########

  These surveys come with dozens of datasets/modules. Filter for relevancy. This step includes translation.
  """
  # spec_file = f"{UPLOAD_FOLDER}/task_description"
  # dataset_file = f"{UPLOAD_FOLDER}/data_documentation"
  spec_file = file_paths['task_description']
  dataset_file = file_paths['data_documentation']

  # Apply nest_asyncio for Jupyter
  nest_asyncio.apply()

  selected_sections = ['GSEC1', 'GSEC2', 'GSEC4', 'GSEC9', 'GSEC14', 'GSEC12_1', 'GSEC7_1', 'CSEC1A', 'CSEC2']
  selected_sections = []

  print(f"Analyzing datasets in {dataset_file} to determine relevant ones....")
  print(f"Using {spec_file} to guide 'relevancy'....")
  relevant_files, all_datasets, dataset_names = asyncio.run(analyze_datasets(spec_file, dataset_file))
  print("The selected sections we proceed with are", dataset_names)

  selected_sections = dataset_names

  """######## ------ PART 2: RELEVANT FIELD SELECTION ------- #########

  Within selected datasets, determine output schema. Some of these fields will exist in the data, some will not. This step includes translation.
  data_sections = {
    'GSEC': {'folder_name': ..., 'metadata': ..., 'data_files' = [...]},
    'CSEC': {'folder_name': ..., 'metadata': ..., 'data_files' = [...]},
    ...
  }
  """
  data_sections = file_paths['data_sections']

  fields_dict = {}
  question_map = {}
  section_pdf_page_maps = {}
  list_reasons = []

  llm = init_chat_model("gpt-4o-mini", model_provider="openai")
  mapping_llm = llm.with_structured_output(schema=ColumnMapping)
  selection_llm = llm.with_structured_output(schema=ColumnSelectionMapping)

  # this block of code (eventually will be put in a function) 1) extracts the column code-question mapping
  # and 2) selects relevant fields with reasoning
  for category in data_sections.keys():  # ex. agriculture vs household
    print("Selecting relevant fields from category", category)
    folder_path = data_sections[category]['folder_path']
    # category_prefix = data_sections[category].prefix
    category_prefix = category
    metadata_path = data_sections[category]['metadata']

    # DO ONCE FOR EACH DATA TABLE (SECTION)
    if category in section_pdf_page_maps.keys():  # if the question map already exists, use it
      section_pdf_page_map = section_pdf_page_maps[category]
    else:  # otherwise create from scratch
      # list_of_sections = extract_all_sections(folder_path, category_prefix)
      section_pdf_page_map = map_section_to_page_number(metadata_path)
      print("Determined a mapping of section to metadata page with info on it...", section_pdf_page_map)
      section_pdf_page_maps[category] = section_pdf_page_map

    section_fields_dict = {}
    section_question_mappings = {}
    for section in section_pdf_page_map: # for each section (eventually this list will depend on phase 1)
      full_section_name = category_prefix + section.upper()
      # if full_section_name not in selected_sections:  # only select for selected sections
      #   continue
      print("Working on", full_section_name, "...")
      section_questionnaire_pages = [page.page_content for page in load_pdf(metadata_path, section_pdf_page_map[section])]
      section_text = f"The section that we are focusing on is: {section}. "

      # determine column mapping
      prompt = info_extraction_prompt_template.invoke({"text": '\n\n'.join([section_text, column_mapping_instructions, * section_questionnaire_pages])})
      mapping = mapping_llm.invoke(prompt) # column mapping in the language
      prompt = translation_prompt_template.invoke({"text": mapping})
      mapping = mapping_llm.invoke(prompt) # column mapping in English
      section_question_mappings[mapping.section_name] = mapping

      # now, select relevant fields
      prompt = field_selection_prompt_template.invoke({"text": '\n\n'.join([field_selection_instructions, str(mapping)])})
      print("prompt", prompt)
      fields_with_reasoning = selection_llm.invoke(prompt)
      print("Selected fields, with reasoning:", fields_with_reasoning, '\n')
      section_fields_dict[mapping.section_name] = fields_with_reasoning
      list_reasons.extend(fields_with_reasoning.column_selection_mapping)

    fields_dict[category] = section_fields_dict
    question_map[category] = section_question_mappings

    print("Our dict of sections containing fields + selection reasoning: ", section_fields_dict)

  ######## POST-PROCESSING INTO DFs ########
  OUTPUT_METADATA_PATH = 'metadata'
  os.makedirs(os.path.join(output_dir, OUTPUT_METADATA_PATH), exist_ok=True)

  # field selection
  # list_reasons = []
  # for section in section_fields_dict.keys():
  #   list_reasons.extend(section_fields_dict[section].column_selection_mapping)
  summary_df = pd.DataFrame.from_records(vars(o) for o in list_reasons)
  
  # save to result and static folders
  summary_csv_path = os.path.join(output_dir, OUTPUT_METADATA_PATH, SUMMARY_PATH)
  summary_df.to_csv(summary_csv_path)
  summary_static_path = os.path.join('static', 'summary.csv')
  summary_df.to_csv(summary_static_path)

  # question maps
  question_map_parsed_dict = {}
  for category in question_map.keys():
    for section in question_map[category].keys():
      list_mappings = question_map[category][section].mappings
      for map in list_mappings:
        if len(map.split('–')) == 2:
          code, question = map.split('–')
        else:
          code, question = map, map
        question_map_parsed_dict[code] = [category, section, question]
  cur_question_map_df = pd.DataFrame.from_dict(question_map_parsed_dict, orient='index')
  cur_question_map_path = os.path.join(output_dir, OUTPUT_METADATA_PATH, f'{category}_question_map.csv')
  cur_question_map_df.to_csv(cur_question_map_path)

  """# *Field Selection Output:*"""

  print(f"\nSnipped of fields selected with reasoning. Results saved to {summary_csv_path}")

  print(f"\nMap of column code to question asked. Results saved to {cur_question_map_path}")

  return {
        'success': True,
        'summary_csv': summary_csv_path,
        'question_map_csv': cur_question_map_path,
        'selected_sections': selected_sections,
        'field_summary': summary_df.head(10).to_dict(orient='records')
    }

def run_part_3_transform_data(file_paths, output_dir):
    import pandas as pd
    import os

    summary_csv_path = file_paths['summary_csv']
    selected_sections = file_paths['selected_sections']
    summary_df = pd.read_csv(summary_csv_path)

    columns_to_keep = list(summary_df[summary_df['is_selected'] == True]['column_code'])
    columns_to_keep_corrected = columns_to_keep.copy()

    merge_keys = ['player_handle', 'uid', 'user_id', 'player_id']
    columns_to_keep_corrected += [k for k in merge_keys if k not in columns_to_keep_corrected]

    print("List of (fuzzy) column codes to keep:", columns_to_keep_corrected)

    result = None
    data_sections = file_paths['data_sections']

    for category in data_sections:
        folder_path = data_sections[category]['folder_path']
        for data_file in os.listdir(folder_path):
            if not data_file.endswith(".csv"):
                continue

            print(f"Considering: {data_file}")
            df = pd.read_csv(os.path.join(folder_path, data_file))

            # ✅ Normalize all ID columns to 'player_id'
            alias_keys = {
                'player_code': 'player_id',
                'user_id': 'player_id',
                'uid': 'player_id',
                'player_handle': 'player_id'
            }
            for old, new in alias_keys.items():
                if old in df.columns:
                    df.rename(columns={old: new}, inplace=True)
                if result is not None and old in result.columns:
                    result.rename(columns={old: new}, inplace=True)

            # Ensure we keep the desired columns plus the merge key
            current_columns = list(set(columns_to_keep_corrected + ['player_id']).intersection(df.columns))
            filtered_df = df[current_columns]

            # ✅ Merge logic using 'player_id'
            if result is None:
                result = filtered_df
            else:
                if 'player_id' not in result.columns or 'player_id' not in filtered_df.columns:
                    print(f"⚠️ 'player_id' missing in one of the datasets. Skipping {data_file}.")
                    continue
                print(f"Merging on key: player_id")
                result = result.merge(filtered_df, on='player_id', how='outer')

    if result is not None:
        group_key = next((k for k in merge_keys if k in result.columns), None)
        if group_key:
            columns_to_agg = [
                col for col in result.select_dtypes(include='number').columns
                if col != group_key
            ]
            result = result.groupby(group_key)[columns_to_agg].sum().reset_index()
            print("Aggregated on:", group_key)
        else:
            print("⚠️ No valid group key found for aggregation.")

        result = result.dropna(axis=1, how='all').drop_duplicates()

        # move key to front if exists
        for key in merge_keys:
            if key in result.columns:
                col = result.pop(key)
                result.insert(0, key, col)

        print("Filtered, aggregated, + merged dataset:")
        print(result.head(10))

        result_path = os.path.join(output_dir, 'generated_dataset.csv')
        result.to_csv(result_path, index=False)
        return {'success': True, 'filename': 'generated_dataset.csv'}

    else:
        print("no similarities found :(")
        return {'success': False, 'message': 'No similarities found. Dataset was not generated.'}


  # result = pd.read_csv('generated_dataset.csv')
  # reverse_map = {v: k for k, v in col_rename_map.items()}
  # result.rename(columns=reverse_map, inplace=True)
  # result.head()

  # from typing import Optional, List
  # from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
  # from pydantic import BaseModel, Field


  # # run the code
  # llm = init_chat_model("gpt-4o-mini", model_provider="openai")
  # coding_llm = llm.with_structured_output(schema=FunctionalCode)

  # columns_kept = 'These are the columns in our dataset: ' + ', '.join(
  #     [f'({key}, {item})' for (key, item) in col_rename_map.items() if key in columns_to_keep_corrected and key in result.columns]
  # )
  # print(columns_kept)
  # prompt = aggregation_code_gen_template.invoke({"text": '\n\n'.join([aggregation_instructions, columns_kept])})
  # print(prompt)
  # code = coding_llm.invoke(prompt) # column mapping in the language
  # print(code)
  # # code_run_output = run_code_and_capture_df(code, "agg_df")
  # # call function to convert run the code

  # full_code_string = code.imports.strip().replace('\n ', '\n') + '\n\n' + code.code.strip().replace('\n ', '\n')
  # print(full_code_string)
  # code_run_output = run_code_and_capture_df(full_code_string, "agg_df")
  # code_run_output

  # df = pd.read_csv('generated_dataset.csv')
  # df.head()

  # """# Part 4: Data Cleaning

  # Not implemented. Will include:
  # - deduplication
  # - selection of highest-quality data points
  # - normalization across data sources.
  # """

  # return {'success': True, 'filename': 'generated_dataset.csv'}