# -*- coding: utf-8 -*-
"""demo_yc_uganda.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19dKm5BOZU5Wq5j6F_hRCTO0psN6j04wq

# Installations & Env Setup
"""

import pandas as pd
import os
from utils.pipeline_utils import load_pdf, extract_all_sections, load_xls, sheet_to_text, map_section_to_page_number, parquet_viewer, run_code_and_capture_df
import google.generativeai as genai
import nest_asyncio
import asyncio

from langchain_templates import info_extraction_prompt_template, translation_prompt_template, field_selection_prompt_template, aggregation_code_gen_template
from models.models import Category
from models.return_models import ColumnMapping, ColumnSelectionMapping, FunctionalCode
from instructions import column_mapping_instructions, field_selection_instructions, aggregation_instructions

from langchain.chat_models import init_chat_model

from modules.p1 import analyze_datasets, find_relevant_datasets

UPLOAD_FOLDER = os.getenv("UPLOAD_FOLDER") or 'uploads'
SUMMARY_PATH = 'summary.csv'
STATIC_FOLDER = os.getenv("STATIC_FOLDER") or 'static'

def run_part_1_2_module_field_selection(file_paths, output_dir):
  """ Check inputs  """
  for path in file_paths:
        print(f"Processing {path}...")

  """######## ------ PART 2: RELEVANT MODULE SELECTION ------- #########

  These surveys come with dozens of datasets/modules. Filter for relevancy. This step includes translation.
  """
  spec_file = f"{UPLOAD_FOLDER}/task_description"
  dataset_file = f"{UPLOAD_FOLDER}/data_documentation"

  # Apply nest_asyncio for Jupyter
  nest_asyncio.apply()

  selected_sections = ['GSEC1', 'GSEC2', 'GSEC4', 'GSEC9', 'GSEC14', 'GSEC12_1', 'GSEC7_1', 'CSEC1A', 'CSEC2']

  print(f"Analyzing datasets in {dataset_file} to determine relevant ones....")
  print(f"Using {spec_file} to guide 'relevancy'....")
  results = asyncio.run(analyze_datasets(spec_file, dataset_file))
  print("The selected sections we proceed with are", results)



  """######## ------ PART 2: RELEVANT FIELD SELECTION ------- #########

  Within selected datasets, determine output schema. Some of these fields will exist in the data, some will not. This step includes translation.
  data_sections = {
    'GSEC': {'folder_name': ..., 'metadata': ..., 'data_files' = [...]},
    'CSEC': {'folder_name': ..., 'metadata': ..., 'data_files' = [...]},
    ...
  }
  """
  data_sections = file_paths['data_sections']

  fields_dict = {}
  question_map = {}
  section_pdf_page_maps = {}

  llm = init_chat_model("gpt-4o-mini", model_provider="openai")
  mapping_llm = llm.with_structured_output(schema=ColumnMapping)
  selection_llm = llm.with_structured_output(schema=ColumnSelectionMapping)

  # this block of code (eventually will be put in a function) 1) extracts the column code-question mapping
  # and 2) selects relevant fields with reasoning
  for category in data_sections.keys():  # ex. agriculture vs household
    print("Selecting relevant fields from category", category)
    folder_path = data_sections[category]['folder_path']
    # category_prefix = data_sections[category].prefix
    category_prefix = category
    metadata_path = data_sections[category]['metadata']

    # DO ONCE FOR EACH DATA TABLE (SECTION)
    if category in section_pdf_page_maps.keys():  # if the question map already exists, use it
      section_pdf_page_map = section_pdf_page_maps[category]
    else:  # otherwise create from scratch
      # list_of_sections = extract_all_sections(folder_path, category_prefix)
      section_pdf_page_map = map_section_to_page_number(metadata_path)
      print("Determined a mapping of section to metadata page with info on it...", section_pdf_page_map)
      section_pdf_page_maps[category] = section_pdf_page_map

    section_fields_dict = {}
    section_question_mappings = {}
    for section in section_pdf_page_map: # for each section (eventually this list will depend on phase 1)
      full_section_name = category_prefix + section.upper()
      if full_section_name not in selected_sections:  # only select for selected sections
        continue
      print("Working on", full_section_name, "...")
      section_questionnaire_pages = [page.page_content for page in load_pdf(metadata_path, section_pdf_page_map[section])]
      section_text = f"The section that we are focusing on is: {section}. "

      # determine column mapping
      prompt = info_extraction_prompt_template.invoke({"text": '\n\n'.join([section_text, column_mapping_instructions, * section_questionnaire_pages])})
      mapping = mapping_llm.invoke(prompt) # column mapping in the language
      prompt = translation_prompt_template.invoke({"text": mapping})
      mapping = mapping_llm.invoke(prompt) # column mapping in English
      section_question_mappings[mapping.section_name] = mapping

      # now, select relevant fields
      prompt = field_selection_prompt_template.invoke({"text": '\n\n'.join([field_selection_instructions, str(mapping)])})
      fields_with_reasoning = selection_llm.invoke(prompt)
      # print("Selected fields, with reasoning:", fields_with_reasoning, '\n')
      section_fields_dict[mapping.section_name] = fields_with_reasoning

    fields_dict[category] = section_fields_dict
    question_map[category] = section_question_mappings

    print("Our dict of sections containing fields + selection reasoning: ", section_fields_dict)

  ######## POST-PROCESSING INTO DFs ########
  OUTPUT_METADATA_PATH = 'metadata'
  os.makedirs(os.path.join(output_dir, OUTPUT_METADATA_PATH), exist_ok=True)

  # field selection
  list_reasons = []
  for section in section_fields_dict.keys():
    list_reasons.extend(section_fields_dict[section].column_selection_mapping)
  summary_df = pd.DataFrame.from_records(vars(o) for o in list_reasons)
  
  # save to result and static folders
  summary_csv_path = os.path.join(output_dir, OUTPUT_METADATA_PATH, SUMMARY_PATH)
  summary_df.to_csv(summary_csv_path)
  summary_static_path = os.path.join('static', 'summary.csv')
  summary_df.to_csv(summary_static_path)

  # question maps
  question_map_parsed_dict = {}
  for category in question_map.keys():
    for section in question_map[category].keys():
      list_mappings = question_map[category][section].mappings
      for map in list_mappings:
        if len(map.split('–')) == 2:
          code, question = map.split('–')
        else:
          code, question = map, map
        question_map_parsed_dict[code] = [category, section, question]
  cur_question_map_df = pd.DataFrame.from_dict(question_map_parsed_dict, orient='index')
  cur_question_map_path = os.path.join(output_dir, OUTPUT_METADATA_PATH, f'{category}_question_map.csv')
  cur_question_map_df.to_csv(cur_question_map_path)

  """# *Field Selection Output:*"""

  print(f"\nSnipped of fields selected with reasoning. Results saved to {summary_csv_path}")

  print(f"\nMap of column code to question asked. Results saved to {cur_question_map_path}")

  return {
        'success': True,
        'summary_csv': summary_csv_path,
        'question_map_csv': cur_question_map_path,
        'selected_sections': selected_sections,
        'field_summary': summary_df.head(10).to_dict(orient='records')
    }

def run_part_3_transform_data(file_paths, output_dir):
  """# Part 3: Transform Data

  Given the data, mapping metadata, and final schema, perform data transformations to generate a dataset. This steps seeks external data if needed (e.g. currency conversions, cost lookup).
  """

  # if summary_df has already been saved to a csv, no need to rerun everything
  # should not really need this part, but maybe for when we submit multiple jobs
  summary_df = None
  summary_csv_path = file_paths['summary_csv']
  if os.path.exists(summary_csv_path):
    summary_df = pd.read_csv(summary_csv_path)
    selected_sections = ['GSEC1', 'GSEC2', 'GSEC4', 'GSEC9', 'GSEC14', 'GSEC12_1', 'GSEC7_1', 'CSEC1A', 'CSEC2']

  # CORRECT COLUMNS TO ALLOW FOR MULTIPLE CODING FORMATS (different prefixes)
  columns_to_keep = list(summary_df[summary_df['is_selected'] == True]['column_code'])
  columns_to_keep_corrected = columns_to_keep
  # columns_to_keep_corrected = [column.replace('0', '') if column.find('0') == 1 else column for column in columns_to_keep ]
  # columns_to_keep_corrected.extend([column.replace('0', '').replace('s', 'h') if column.find('0') == 1 else column for column in columns_to_keep])
  columns_to_keep_corrected.extend(['hhid', 'PID', 'pid_unps', 't0_hhid', 'EA_code', 't0_EA_code', 'interview__key'])
  col_rename_map = dict(zip(summary_df['column_code'], summary_df['column_question']))
  print("List of (fuzzy) column codes to keep:", columns_to_keep_corrected, "\n")
  print("Also renaming columns based on this map:", col_rename_map)

  result = None
  data_sections = file_paths['data_sections']
  # first, from data folder, only get datasets that were selected
  for category in data_sections.keys():
    folder_path = data_sections[category]['folder_path']
    all_data_files = os.listdir(folder_path)
    for data_file in all_data_files:
      print(f"Considering: {data_file}...")
      # hacky, but this is how we match CSEC1 format
      if data_file.replace('.csv', '').replace('_', '') not in selected_sections:
        continue
      # proceed with dataset
      print(f"Merging with {data_file}")
      data_file = pd.read_csv(f'{folder_path}/{data_file}')
      print(data_file.head())
      # only keep columns we are supposed to
      current_columns_to_keep = list(set(columns_to_keep_corrected).intersection(data_file.columns))
      filtered_df_corrected = data_file[current_columns_to_keep]
      # join the datasets
      # Determine available merge keys
      if result is None:
         result = filtered_df_corrected
      else:
        merge_key = next((k for k in ['hhid', 'EA_code'] if k in result.columns and k in filtered_df_corrected.columns), None)
        if not merge_key:
            print(f"⚠️ No common key to merge on for {data_file}. Skipping.")
            continue
        result = result.merge(filtered_df_corrected, on=merge_key, how='outer')
        # merge_keys = ['hhid', 'EA_code']
        # left_keys = [k for k in merge_keys if k in result.columns] if result is not None else []
        # right_keys = [k for k in merge_keys if k in filtered_df_corrected.columns]

        # # Try merging on shared key
        # merged = None
        # for lk in left_keys:
        #     for rk in right_keys:
        #         try:
        #             merged = result.merge(filtered_df_corrected, left_on=lk, right_on=rk, how='outer') if result is not None else filtered_df_corrected
        #             print(f"Merged on {lk} ⇄ {rk}")
        #             break
        #         except Exception as e:
        #             continue
        #     if merged is not None:
        #         break

        # if merged is not None:
        #     result = merged
        # else:
        #     print(f"⚠️ Could not merge with: {filtered_df_corrected.columns.tolist()}")
        #     continue

  if result is not None:
    group_key = 'hhid' if 'hhid' in result.columns else 'EA_code' if 'EA_code' in result.columns else None
    if group_key:
        columns_to_agg = [col for col in result.columns if col.startswith(('h', 's')) and col != group_key]
        result = result.groupby(group_key)[columns_to_agg].sum().reset_index()
    else:
        print("⚠️ No valid group key found for final aggregation.")

      # if result is None:
      #   result = filtered_df_corrected
      # else:
      #   try:
      #     result = result.merge(filtered_df_corrected, left_on='hhid', right_on='hhid', how='outer')
      #   except:
      #     try:
      #       result = result.merge(filtered_df_corrected, left_on='EA_code', right_on='hhid', how='outer')
      #     except:
      #       try:
      #         result = result.merge(filtered_df_corrected, left_on='hhid', right_on='EA_code', how='outer')
      #       except:
      #         try:
      #           result = result.merge(filtered_df_corrected, left_on='EA_code', right_on='EA_code', how='outer')
      #           print("did this")
      #         except:
      #           print(filtered_df_corrected.columns, result.columns)

      # columns_to_agg = [col for col in current_columns_to_keep if col.startswith('h') or col.startswith('s') and col != 'hhid']
      # try:
      #   result = result.groupby('hhid')[columns_to_agg].sum().reset_index(drop=False)
      # except:
      #   try:
      #     result = result.groupby('EA_code')[columns_to_agg].sum().reset_index(drop=False)
      #     print("did this too")
      #   except:
      #     continue

    result.rename(columns=col_rename_map, inplace=True)
    result.rename(columns={'hhid_y':'hhid'}, inplace=True)
    result = result.dropna(axis=1, how='all') # drop columns where all values are NaN
    result = result.drop(columns=[col for col in ['hhid_x'] if col in result.columns])
    result = result.drop_duplicates()

    for key in ['hhid', 'EA_code']:
      if key in result.columns:
          col = result.pop(key)
          result.insert(0, key, col) # insert key col at beginning

    # import ace_tools as tools; tools.display_dataframe_to_user(name="Filtered Household Data", dataframe=filtered_df_corrected)
    print("Filtered, aggregated, + renamed dataset...")
    result.head(10)
    # ADD QUESTION TO HEADER

    """# *Filtered + Aggregated + Merged Dataset Output*"""

    print("List of (fuzzy) column codes to keep:", columns_to_keep_corrected, "\n")
    print("Also renaming columns based on this map:", col_rename_map, "\n")
    print("List of columns that were aggregated per household:", columns_to_agg, '\n')
    
    # save!
    result_path = os.path.join(output_dir, 'generated_dataset.csv')
    result.to_csv(result_path)
  else: # if no result, return success with fail message
    print("no similarities found :(")
    return {'success': False, 'message': 'No similarities found. Dataset was not generated.'}


  result = pd.read_csv('generated_dataset.csv')
  reverse_map = {v: k for k, v in col_rename_map.items()}
  result.rename(columns=reverse_map, inplace=True)
  result.head()

  from typing import Optional, List
  from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
  from pydantic import BaseModel, Field


  # run the code
  llm = init_chat_model("gpt-4o-mini", model_provider="openai")
  coding_llm = llm.with_structured_output(schema=FunctionalCode)

  columns_kept = 'These are the columns in our dataset: ' + ', '.join(
      [f'({key}, {item})' for (key, item) in col_rename_map.items() if key in columns_to_keep_corrected and key in result.columns]
  )
  print(columns_kept)
  prompt = aggregation_code_gen_template.invoke({"text": '\n\n'.join([aggregation_instructions, columns_kept])})
  print(prompt)
  code = coding_llm.invoke(prompt) # column mapping in the language
  print(code)
  # code_run_output = run_code_and_capture_df(code, "agg_df")
  # call function to convert run the code

  full_code_string = code.imports.strip().replace('\n ', '\n') + '\n\n' + code.code.strip().replace('\n ', '\n')
  print(full_code_string)
  code_run_output = run_code_and_capture_df(full_code_string, "agg_df")
  code_run_output

  df = pd.read_csv('generated_dataset.csv')
  df.head()

  """# Part 4: Data Cleaning

  Not implemented. Will include:
  - deduplication
  - selection of highest-quality data points
  - normalization across data sources.
  """

  return {'success': True, 'filename': 'generated_dataset.csv'}