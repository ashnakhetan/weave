# -*- coding: utf-8 -*-
"""demo_yc_uganda.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19dKm5BOZU5Wq5j6F_hRCTO0psN6j04wq

# Installations & Env Setup
"""

import pandas as pd
import os
from utils import load_pdf, extract_all_sections, load_xls, sheet_to_text, map_section_to_page_number, parquet_viewer, run_code_and_capture_df
import google.generativeai as genai
import nest_asyncio
import asyncio

from langchain_templates import info_extraction_prompt_template, translation_prompt_template, field_selection_prompt_template, aggregation_code_gen_template
from models.models import Category
from models.return_models import ColumnMapping, ColumnSelectionMapping, FunctionalCode
from instructions import column_mapping_instructions, field_selection_instructions, aggregation_instructions

from langchain.chat_models import init_chat_model

from modules.p1 import analyze_datasets, find_relevant_datasets

UPLOAD_FOLDER = os.getenv("UPLOAD_FOLDER") or 'uploads'
SUMMARY_PATH = 'summary.csv'

def run(file_paths, output_dir):
  """ Check inputs  """
  for path in file_paths:
        print(f"Processing {path}...")

  """######## ------ PART 2: RELEVANT MODULE SELECTION ------- #########

  These surveys come with dozens of datasets/modules. Filter for relevancy. This step includes translation.
  """
  spec_file = f"{UPLOAD_FOLDER}/task_description"
  dataset_file = f"{UPLOAD_FOLDER}/data_documentation"

  # Apply nest_asyncio for Jupyter
  nest_asyncio.apply()

  selected_sections = ['GSEC1', 'GSEC2', 'GSEC4', 'GSEC9', 'GSEC14', 'GSEC12_1', 'GSEC7_1', 'CSEC1A', 'CSEC2']

  print(f"Analyzing datasets in {dataset_file} to determine relevant ones....")
  print(f"Using {spec_file} to guide 'relevancy'....")
  results = asyncio.run(analyze_datasets(spec_file, dataset_file))
  print("The selected sections we proceed with are", results)



  """######## ------ PART 2: RELEVANT FIELD SELECTION ------- #########

  Within selected datasets, determine output schema. Some of these fields will exist in the data, some will not. This step includes translation.
  data_sections = {
    'GSEC': {'folder_name': ..., 'metadata': ..., 'data_files' = [...]},
    'CSEC': {'folder_name': ..., 'metadata': ..., 'data_files' = [...]},
    ...
  }
  """
  data_sections = file_paths['data_sections']

  fields_dict = {}
  question_map = {}
  section_pdf_page_maps = {}

  llm = init_chat_model("gpt-4o-mini", model_provider="openai")
  mapping_llm = llm.with_structured_output(schema=ColumnMapping)
  selection_llm = llm.with_structured_output(schema=ColumnSelectionMapping)

  # this block of code (eventually will be put in a function) 1) extracts the column code-question mapping
  # and 2) selects relevant fields with reasoning
  for category in data_sections.keys():  # ex. agriculture vs household
    print("Selecting relevant fields from category", category)
    folder_path = data_sections[category]['folder_path']
    # category_prefix = data_sections[category].prefix
    category_prefix = category
    metadata_path = data_sections[category]['metadata']

    # DO ONCE FOR EACH DATA TABLE (SECTION)
    if category in section_pdf_page_maps.keys():  # if the question map already exists, use it
      section_pdf_page_map = section_pdf_page_maps[category]
    else:  # otherwise create from scratch
      # list_of_sections = extract_all_sections(folder_path, category_prefix)
      section_pdf_page_map = map_section_to_page_number(metadata_path)
      print("Determined a mapping of section to metadata page with info on it...", section_pdf_page_map)
      section_pdf_page_maps[category] = section_pdf_page_map

    section_fields_dict = {}
    section_question_mappings = {}
    for section in section_pdf_page_map: # for each section (eventually this list will depend on phase 1)
      full_section_name = category_prefix + section.upper()
      if full_section_name not in selected_sections:  # only select for selected sections
        continue
      print("Working on", full_section_name, "...")
      section_questionnaire_pages = [page.page_content for page in load_pdf(metadata_path, section_pdf_page_map[section])]
      section_text = f"The section that we are focusing on is: {section}. "

      # determine column mapping
      prompt = info_extraction_prompt_template.invoke({"text": '\n\n'.join([section_text, column_mapping_instructions, * section_questionnaire_pages])})
      mapping = mapping_llm.invoke(prompt) # column mapping in the language
      prompt = translation_prompt_template.invoke({"text": mapping})
      mapping = mapping_llm.invoke(prompt) # column mapping in English
      section_question_mappings[mapping.section_name] = mapping

      # now, select relevant fields
      prompt = field_selection_prompt_template.invoke({"text": '\n\n'.join([field_selection_instructions, str(mapping)])})
      fields_with_reasoning = selection_llm.invoke(prompt)
      # print("Selected fields, with reasoning:", fields_with_reasoning, '\n')
      section_fields_dict[mapping.section_name] = fields_with_reasoning

    fields_dict[category] = section_fields_dict
    question_map[category] = section_question_mappings

    print("Our dict of sections containing fields + selection reasoning: ", section_fields_dict)

  ######## POST-PROCESSING INTO DFs ########
  OUTPUT_METADATA_PATH = 'metadata'
  os.makedirs(os.path.join(output_dir, OUTPUT_METADATA_PATH), exist_ok=True)

  # field selection
  list_reasons = []
  for section in section_fields_dict.keys():
    list_reasons.extend(section_fields_dict[section].column_selection_mapping)
  summary_df = pd.DataFrame.from_records(vars(o) for o in list_reasons)
  summary_csv_path = os.path.join(output_dir, OUTPUT_METADATA_PATH, SUMMARY_PATH)
  summary_df.to_csv(summary_csv_path)

  # question maps
  question_map_parsed_dict = {}
  for category in question_map.keys():
    for section in question_map[category].keys():
      list_mappings = question_map[category][section].mappings
      for map in list_mappings:
        if len(map.split('–')) == 2:
          code, question = map.split('–')
        else:
          code, question = map, map
        question_map_parsed_dict[code] = [category, section, question]
  cur_question_map_df = pd.DataFrame.from_dict(question_map_parsed_dict, orient='index')
  cur_question_map_path = os.path.join(output_dir, OUTPUT_METADATA_PATH, f'{category}_question_map.csv')
  cur_question_map_df.to_csv(cur_question_map_path)

  """# *Field Selection Output:*"""

  print(f"Snipped of fields selected with reasoning. Results saved to {summary_csv_path}")
  summary_df.head(5)

  print(f"Map of column code to question asked. Results saved to {cur_question_map_path}")
  cur_question_map_df.tail()

  """# Part 3: Transform Data

  Given the data, mapping metadata, and final schema, perform data transformations to generate a dataset. This steps seeks external data if needed (e.g. currency conversions, cost lookup).
  """

  # if summary_df has already been saved to a csv, no need to rerun everything
  # should not really need this part, but maybe for when we submit multiple jobs
  # summary_df = None
  # summary_csv_path = os.path.join(output_dir, SUMMARY_PATH)
  # if os.path.exists(summary_csv_path):
  #   summary_df = pd.read_csv(summary_csv_path)
  #   selected_sections = ['GSEC1', 'GSEC2', 'GSEC4', 'GSEC9', 'GSEC14', 'GSEC12_1', 'GSEC7_1', 'CSEC1A', 'CSEC2']

  #   folder_to_questionnaire = {
  #     # 'agriculture': Category('Agric', 'AGSEC',  'agriculture_qx.pdf'),
  #     'community': Category('Community', 'CSEC',  "unps_community.pdf"),
  #     'household': Category('HH', 'GSEC',  "unps_hhq.pdf"),
  #     # 'woman': Category('Woman', 'WSEC',  "unps_woman.pdf"),
  #   }
  # summary_df.head()

  # CORRECT COLUMNS TO ALLOW FOR MULTIPLE CODING FORMATS (different prefixes)
  columns_to_keep = list(summary_df[summary_df['is_selected'] == True]['column_code'])
  columns_to_keep_corrected = columns_to_keep
  # columns_to_keep_corrected = [column.replace('0', '') if column.find('0') == 1 else column for column in columns_to_keep ]
  # columns_to_keep_corrected.extend([column.replace('0', '').replace('s', 'h') if column.find('0') == 1 else column for column in columns_to_keep])
  columns_to_keep_corrected.extend(['hhid', 'PID', 'pid_unps', 't0_hhid', 'EA_code', 't0_EA_code', 'interview__key'])
  col_rename_map = dict(zip(summary_df['column_code'], summary_df['column_question']))
  print("List of (fuzzy) column codes to keep:", columns_to_keep_corrected, "\n")
  print("Also renaming columns based on this map:", col_rename_map)

  result = None
  # first, from data folder, only get datasets that were selected
  for category in data_sections.keys():
    folder_path = data_sections[category]['folder_path']
    all_data_files = os.listdir(folder_path)
    for data_file in all_data_files:
      if data_file.replace('.csv', '') not in selected_sections:
        continue
      # proceed with dataset
      data_file_csv = pd.read_csv(f'{folder_path}/{data_file}')
      print(data_file_csv.head())
      # only keep columns we are supposed to
      current_columns_to_keep = list(set(columns_to_keep_corrected).intersection(data_file_csv.columns))
      filtered_df_corrected = data_file_csv[current_columns_to_keep]
  # join the datasets
      if result is None:
        result = filtered_df_corrected
      else:
        try:
          result = result.merge(filtered_df_corrected, left_on='hhid', right_on='hhid', how='outer')
        except:
          try:
            result = result.merge(filtered_df_corrected, left_on='EA_code', right_on='hhid', how='outer')
          except:
            try:
              result = result.merge(filtered_df_corrected, left_on='hhid', right_on='EA_code', how='outer')
            except:
              try:
                result = result.merge(filtered_df_corrected, left_on='EA_code', right_on='EA_code', how='outer')
              except:
                print(filtered_df_corrected.columns, result.columns)

      columns_to_agg = [col for col in current_columns_to_keep if col.startswith('h') or col.startswith('s') and col != 'hhid']
      try:
        result = result.groupby('hhid')[columns_to_agg].sum().reset_index(drop=False)
      except:
        try:
          result = result.groupby('EA_code')[columns_to_agg].sum().reset_index(drop=False)
        except:
          continue


  result.rename(columns=col_rename_map, inplace=True)
  result.rename(columns={'hhid_y':'hhid'}, inplace=True)
  result = result.dropna(axis=1, how='all') # drop columns where all values are NaN
  result = result.drop(columns=['hhid_x', 'EA_code'])
  result = result.drop_duplicates()

  column_to_move = result.pop('hhid')
  result.insert(0, 'hhid', column_to_move) #inserts hhid at the beginning

  # import ace_tools as tools; tools.display_dataframe_to_user(name="Filtered Household Data", dataframe=filtered_df_corrected)
  print("Filtered, aggregated, + renamed dataset...")
  result.head(10)
  # ADD QUESTION TO HEADER

  """# *Filtered + Aggregated + Merged Dataset Output*"""

  print("List of (fuzzy) column codes to keep:", columns_to_keep_corrected, "\n")
  print("Also renaming columns based on this map:", col_rename_map, "\n")
  print("List of columns that were aggregated per household:", columns_to_agg, '\n')
  result.head(10)

  result.to_csv('generated_dataset.csv')

  result = pd.read_csv('generated_dataset.csv')
  reverse_map = {v: k for k, v in col_rename_map.items()}
  result.rename(columns=reverse_map, inplace=True)
  result.head()

  from typing import Optional, List
  from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
  from pydantic import BaseModel, Field


  # run the code
  llm = init_chat_model("gpt-4o-mini", model_provider="openai")
  coding_llm = llm.with_structured_output(schema=FunctionalCode)

  columns_kept = 'These are the columns in our dataset: ' + ', '.join(
      [f'({key}, {item})' for (key, item) in col_rename_map.items() if key in columns_to_keep_corrected and key in result.columns]
  )
  print(columns_kept)
  prompt = aggregation_code_gen_template.invoke({"text": '\n\n'.join([aggregation_instructions, columns_kept])})
  print(prompt)
  code = coding_llm.invoke(prompt) # column mapping in the language
  print(code)
  # code_run_output = run_code_and_capture_df(code, "agg_df")
  # call function to convert run the code

  full_code_string = code.imports.strip().replace('\n ', '\n') + '\n\n' + code.code.strip().replace('\n ', '\n')
  print(full_code_string)
  code_run_output = run_code_and_capture_df(full_code_string, "agg_df")
  code_run_output

  df = pd.read_csv('generated_dataset.csv')
  df.head()

  """Compare this to the student's work:"""

  pd.read_parquet("../summary.parquet").head(123)

  pd.read_parquet('../uganda_full.parquet')

  """# Part 4: Data Cleaning

  Not implemented. Will include:
  - deduplication
  - selection of highest-quality data points
  - normalization across data sources.
  """

